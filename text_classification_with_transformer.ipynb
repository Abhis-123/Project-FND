{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:03:54.396288Z",
     "iopub.status.busy": "2021-09-04T05:03:54.395970Z",
     "iopub.status.idle": "2021-09-04T05:03:54.400152Z",
     "shell.execute_reply": "2021-09-04T05:03:54.399205Z",
     "shell.execute_reply.started": "2021-09-04T05:03:54.396257Z"
    },
    "id": "E0TZNudwJFRv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:03:54.827134Z",
     "iopub.status.busy": "2021-09-04T05:03:54.826814Z",
     "iopub.status.idle": "2021-09-04T05:03:54.831415Z",
     "shell.execute_reply": "2021-09-04T05:03:54.830462Z",
     "shell.execute_reply.started": "2021-09-04T05:03:54.827105Z"
    }
   },
   "outputs": [],
   "source": [
    "## cleaning text \n",
    "import nltk ,re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:03:55.696139Z",
     "iopub.status.busy": "2021-09-04T05:03:55.695822Z",
     "iopub.status.idle": "2021-09-04T05:03:55.703578Z",
     "shell.execute_reply": "2021-09-04T05:03:55.702618Z",
     "shell.execute_reply.started": "2021-09-04T05:03:55.696110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defining the object for Lemmatization\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4A3G1Ps_JFRw"
   },
   "source": [
    "## Implement a Transformer block as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:04:02.149939Z",
     "iopub.status.busy": "2021-09-04T05:04:02.149628Z",
     "iopub.status.idle": "2021-09-04T05:04:02.159038Z",
     "shell.execute_reply": "2021-09-04T05:04:02.158287Z",
     "shell.execute_reply.started": "2021-09-04T05:04:02.149912Z"
    },
    "id": "uBIvGU-7JFRw"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzUDpGtOJFRw"
   },
   "source": [
    "## Implement embedding layer\n",
    "\n",
    "Two seperate embedding layers, one for tokens, one for token index (positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:04:04.248301Z",
     "iopub.status.busy": "2021-09-04T05:04:04.247974Z",
     "iopub.status.idle": "2021-09-04T05:04:04.254920Z",
     "shell.execute_reply": "2021-09-04T05:04:04.253782Z",
     "shell.execute_reply.started": "2021-09-04T05:04:04.248269Z"
    },
    "id": "5gzpPAhDJFRx"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgOw9SRWJFRx"
   },
   "source": [
    "## prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:04:24.697409Z",
     "iopub.status.busy": "2021-09-04T05:04:24.697087Z",
     "iopub.status.idle": "2021-09-04T05:04:25.677576Z",
     "shell.execute_reply": "2021-09-04T05:04:25.676724Z",
     "shell.execute_reply.started": "2021-09-04T05:04:24.697377Z"
    },
    "id": "R1CFzx6uJFRy"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/fake-news/train.csv\")\n",
    "# drop all null values\n",
    "train= train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:04:26.119979Z",
     "iopub.status.busy": "2021-09-04T05:04:26.119657Z",
     "iopub.status.idle": "2021-09-04T05:04:26.128099Z",
     "shell.execute_reply": "2021-09-04T05:04:26.127115Z",
     "shell.execute_reply.started": "2021-09-04T05:04:26.119950Z"
    }
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords=stopwords.words('english')\n",
    "stemmer=PorterStemmer()\n",
    "# clean unwanted text like stopwords, @(Mention), https(url), #(Hashtag), punctuations\n",
    "def removeUnwantedText(text):\n",
    "    #remove urls\n",
    "    if text == np.NaN or type(text) != str:\n",
    "      text = \" \"\n",
    "    text = re.sub(r'http\\S+', \" \", text)\n",
    "    text = re.sub(r'@\\w+',' ',text)\n",
    "    text = re.sub(r'#\\w+', ' ', text)\n",
    "    text = re.sub('r<.*?>',' ', text)\n",
    "    # html tags\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = \" \".join([word for word in text if not word in stopwords])\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, \"\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:04:27.805667Z",
     "iopub.status.busy": "2021-09-04T05:04:27.805299Z",
     "iopub.status.idle": "2021-09-04T05:04:28.538576Z",
     "shell.execute_reply": "2021-09-04T05:04:28.537569Z",
     "shell.execute_reply.started": "2021-09-04T05:04:27.805634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title']=train['title'].apply(removeUnwantedText)\n",
    "max(train['title'].apply( lambda x : len(x.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:04:28.540555Z",
     "iopub.status.busy": "2021-09-04T05:04:28.540181Z",
     "iopub.status.idle": "2021-09-04T05:04:28.545081Z",
     "shell.execute_reply": "2021-09-04T05:04:28.544219Z",
     "shell.execute_reply.started": "2021-09-04T05:04:28.540519Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text= text.lower()\n",
    "    text= text.split(\" \")\n",
    "    text = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:04:30.063640Z",
     "iopub.status.busy": "2021-09-04T05:04:30.063222Z",
     "iopub.status.idle": "2021-09-04T05:04:30.890899Z",
     "shell.execute_reply": "2021-09-04T05:04:30.890009Z",
     "shell.execute_reply.started": "2021-09-04T05:04:30.063607Z"
    }
   },
   "outputs": [],
   "source": [
    "train['title']=train['title'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:04:31.157331Z",
     "iopub.status.busy": "2021-09-04T05:04:31.157014Z",
     "iopub.status.idle": "2021-09-04T05:04:31.161156Z",
     "shell.execute_reply": "2021-09-04T05:04:31.160062Z",
     "shell.execute_reply.started": "2021-09-04T05:04:31.157300Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = 25000  # Only consider the top 20k words\n",
    "maxlen = 64  # Only consider the first 200 words of each movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:14:27.826870Z",
     "iopub.status.busy": "2021-09-04T05:14:27.826507Z",
     "iopub.status.idle": "2021-09-04T05:14:28.183095Z",
     "shell.execute_reply": "2021-09-04T05:14:28.182234Z",
     "shell.execute_reply.started": "2021-09-04T05:14:27.826835Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "tokenizer = text.Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train[\"title\"])\n",
    "def prep_text(texts, tokenizer, max_sequence_length):\n",
    "    # Turns text into into padded sequences.\n",
    "    text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return sequence.pad_sequences(text_sequences, maxlen=max_sequence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:15:02.426176Z",
     "iopub.status.busy": "2021-09-04T05:15:02.425845Z",
     "iopub.status.idle": "2021-09-04T05:15:02.809316Z",
     "shell.execute_reply": "2021-09-04T05:15:02.808472Z",
     "shell.execute_reply.started": "2021-09-04T05:15:02.426147Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## coverting to matrix \n",
    "x= prep_text(train['title'],tokenizer,maxlen)\n",
    "x= np.array(x)\n",
    "y =np.array(train['label'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSYiXigdJFRy"
   },
   "source": [
    "## Create classifier model using transformer layer\n",
    "\n",
    "Transformer layer outputs one vector for each time step of our input sequence.\n",
    "Here, we take the mean across all time steps and\n",
    "use a feed forward network on top of it to classify text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:16:26.165977Z",
     "iopub.status.busy": "2021-09-04T05:16:26.165650Z",
     "iopub.status.idle": "2021-09-04T05:16:26.323808Z",
     "shell.execute_reply": "2021-09-04T05:16:26.322962Z",
     "shell.execute_reply.started": "2021-09-04T05:16:26.165948Z"
    },
    "id": "lmpImb7bJFRz"
   },
   "outputs": [],
   "source": [
    "\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WELvcmfCJFRz"
   },
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-04T05:16:33.815498Z",
     "iopub.status.busy": "2021-09-04T05:16:33.815157Z",
     "iopub.status.idle": "2021-09-04T05:16:50.288129Z",
     "shell.execute_reply": "2021-09-04T05:16:50.287332Z",
     "shell.execute_reply.started": "2021-09-04T05:16:33.815464Z"
    },
    "id": "1MEXC1N7JFRz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "400/400 [==============================] - 6s 13ms/step - loss: 0.5422 - accuracy: 0.6837 - val_loss: 0.1642 - val_accuracy: 0.9313\n",
      "Epoch 2/3\n",
      "400/400 [==============================] - 5s 12ms/step - loss: 0.0956 - accuracy: 0.9649 - val_loss: 0.1756 - val_accuracy: 0.9360\n",
      "Epoch 3/3\n",
      "400/400 [==============================] - 5s 13ms/step - loss: 0.0260 - accuracy: 0.9943 - val_loss: 0.2263 - val_accuracy: 0.9304\n"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=3, validation_data=(x_test, y_test)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_with_transformer",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
